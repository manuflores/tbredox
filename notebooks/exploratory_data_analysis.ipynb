{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis on TF-overexpression fold-change effect space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Emanuel Flores and Adrian Jinich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.017171Z",
     "start_time": "2020-02-02T05:45:48.684715Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import numba\n",
    "import bebi103\n",
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "import holoviews as hv \n",
    "from sklearn.decomposition import PCA \n",
    "from umap import UMAP\n",
    "import hvplot\n",
    "import hvplot.pandas\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh_catplot\n",
    "import colorcet as cc\n",
    "\n",
    "\n",
    "hv.extension('bokeh')\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "#Setting all the plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#Make the figure format appear as svg\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# Load black magic command for writing w/style\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right. Before proceding to dosupervised learning to predict new putative oxidoreductase or oxidoreductase-like proteins, let's do some exploratory analysis. We'll mainly use dimensionality reduction to see if the oxidoreductases cluster in a reduced version of the fold-change TF overexpression feature space. Let's start by loading our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.168505Z",
     "start_time": "2020-02-02T05:45:48.671Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get path to data\n",
    "path = '../data/'\n",
    "\n",
    "# read dataset\n",
    "df = pd.read_csv(path + 'fold_change_tf_ko_plus_redox_annot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.185127Z",
     "start_time": "2020-02-02T05:45:48.677Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.192005Z",
     "start_time": "2020-02-02T05:45:48.683Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_gene_data(data, gene_name_column, test_gene_list):\n",
    "    \n",
    "    \"\"\"Extract data from specific genes given a larger dataframe.\n",
    "    \n",
    "    Inputs\n",
    "    \n",
    "    * data: large dataframe from where to filter\n",
    "    * gene_name_column: column to filter from\n",
    "    * test_gene_list : a list of genes you want to get\n",
    "    \n",
    "    Output\n",
    "    * dataframe with the genes you want\n",
    "    \"\"\"\n",
    "    \n",
    "    gene_profiles = pd.DataFrame()\n",
    "\n",
    "    for gene in data[gene_name_column].values:\n",
    "\n",
    "        if gene in test_gene_list: \n",
    "\n",
    "            df_ = data[(data[gene_name_column] == gene)]\n",
    "\n",
    "            gene_profiles = pd.concat([gene_profiles, df_])\n",
    "    \n",
    "    gene_profiles.drop_duplicates(inplace = True)\n",
    "    \n",
    "    return gene_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.200784Z",
     "start_time": "2020-02-02T05:45:48.689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get fold change columns\n",
    "fc_cols = [col for col in df.columns if 'FC' in col]\n",
    "\n",
    "# Get FC data using fancy indexing on columns\n",
    "fc_data = df[fc_cols]\n",
    "\n",
    "# Get TF locus tags\n",
    "tf_ids = [fc_cols[i][3:] for i in range(len(fc_cols))]\n",
    "\n",
    "# Get TF data for annotation \n",
    "tf_data = get_gene_data(df, 'Rv_ID', tf_ids)\n",
    "\n",
    "# Save tf_id in original df using sleazy list comprehension\n",
    "df['TF'] = [ 1 if gene in tf_ids else 0 for gene in df.Rv_ID.values]\n",
    "\n",
    "# take a look at FC data\n",
    "fc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, with our dataset in place, we can now proceed to do some visualizations. We'll be using the brand new [`hvplot`](http://hvplot.pyviz.org/) library in order to make simple high-level visualizations using the power of [HoloViews](http://holoviews.org/) and [Bokeh](https://docs.bokeh.org/en/latest/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the fold-change distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, before junmping into making dimension reduction, let's make a simple visualization to get a sense of the distributions of our feature space. In order to do so, we'll have to first \"melt\" our dataset in order to get it in a [tidy format](https://en.wikipedia.org/wiki/Tidy_data). One could say that our data is already tidy in some sense, but specifically we need it in the format such that we can visualize each distribtuion (feature) as a categorical variable and the fold-change values as a numeric value. This is the way we will be able to visualize it using [`seaborn`](http://seaborn.pydata.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.216998Z",
     "start_time": "2020-02-02T05:45:48.695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make tidy data\n",
    "fc_data_melt = pd.melt(fc_data, var_name=\"fold_change_sample\", value_name=\"fold-change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.246258Z",
     "start_time": "2020-02-02T05:45:48.700Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "plt.figure(figsize = (20, 5))\n",
    "\n",
    "\n",
    "sns.boxplot(\n",
    "    data = fc_data_melt, \n",
    "    x ='fold_change_sample', \n",
    "    y = 'fold-change'\n",
    ")\n",
    "\n",
    "# Eliminate xlabel sample names \n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in general the distributions are centered around zero with variances ranging a bit, but not by too much. It might be a good bet to normalize them but for now we'll continue with our analysis. \n",
    "\n",
    "It is important to highlight that the PCA is sensitive to the normalization of the features, but because the data look like evely distributed we're confident with going through as is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction using PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, we'll now proceed to visualize the data using principal component analysis. We'll keep the components that amount to 80% of the variance of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.255242Z",
     "start_time": "2020-02-02T05:45:48.707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize PCA object with 0.8 of the variance of the data\n",
    "pca = PCA(0.8).fit(fc_data)\n",
    "\n",
    "# Project the data into principal components \n",
    "projections = pca.transform(fc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.265004Z",
     "start_time": "2020-02-02T05:45:48.712Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the first two principal components to the original df\n",
    "df['PC1'], df['PC2'] = projections[:, 0], projections[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.272922Z",
     "start_time": "2020-02-02T05:45:48.720Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize FC data on projected in the first two PCs\n",
    "df.hvplot(\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    c=\"redox_enzyme\",\n",
    "    alpha=0.6,\n",
    "    kind=\"scatter\",\n",
    "    padding=0.2, # padding let's you have a nice zoom out as default\n",
    "    cmap=cc.bky, # set colormap using colorcet https://colorcet.holoviz.org\n",
    "    clabel=\"oxired\",\n",
    "    width = 500, \n",
    "    height = 300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot see really much structure in the first two PCs, at the transcriptome level (clusters), nor at the oxidoreductase association level (separation between blue and brown dots).\n",
    "\n",
    "To get more information, we could do a heatmap to visualize which TFs overexpression features have the highest \"loading\" on the first two principal components.\n",
    "\n",
    "The loadings can be loosely thought of the covariance between the original features  (TF overexpression experiments) and the principal components. They incorporate information about the magnitude of the variance explained by each PC and the contribution a given variable have on them. You can read more about that [here](https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.279033Z",
     "start_time": "2020-02-02T05:45:48.726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the principal components \n",
    "components = pca.components_\n",
    "\n",
    "# Get the eigenvalues\n",
    "eigenvals = pca.explained_variance_\n",
    "\n",
    "# Compute the loadings \n",
    "loadings = components.T*np.sqrt(eigenvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we leveraged numpy broadcasting twice here: compute the square root of every eigenvalue and then to multiply each PC by its corresponding eigenvalue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.287045Z",
     "start_time": "2020-02-02T05:45:48.732Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter loadings of first 10 PCs for visualization\n",
    "first_ten_loadings = loadings[:, :10]\n",
    "\n",
    "# Save as a dataframe to visualize with hvplot\n",
    "df_loadings = pd.DataFrame(\n",
    "    first_ten_loadings,\n",
    "    columns=[\"PC \" + str(i) for i in range(1, 11)],\n",
    ")\n",
    "\n",
    "# Reset index for concatenation\n",
    "tf_annotation =  tf_data[['gene_name', 'Function', 'Rv_ID']].reset_index(drop = True)\n",
    "\n",
    "# Merge by size \n",
    "df_loadings_ = pd.concat([df_loadings,tf_annotation], axis = 1)\n",
    "\n",
    "# Set index to FC features\n",
    "df_loadings_.index = fc_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.297997Z",
     "start_time": "2020-02-02T05:45:48.739Z"
    }
   },
   "outputs": [],
   "source": [
    "df_loadings_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.305440Z",
     "start_time": "2020-02-02T05:45:48.744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sort values by first two components\n",
    "df_loadings_.sort_values(by = ['PC 1', 'PC 2'], inplace =True, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.314049Z",
     "start_time": "2020-02-02T05:45:48.749Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep only the first 20 rows (features)\n",
    "df_loadings_plot = df_loadings_.iloc[:20, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.320975Z",
     "start_time": "2020-02-02T05:45:48.756Z"
    }
   },
   "outputs": [],
   "source": [
    "df_loadings_plot.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final heavy-lifting we need to do before plotting the heatmap is melting the dataframe such that we get it in tidy format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.327495Z",
     "start_time": "2020-02-02T05:45:48.763Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tidy plotting df\n",
    "df_plot = pd.melt(\n",
    "    df_loadings_plot,\n",
    "    id_vars=tf_annotation.columns.tolist(),\n",
    "    var_name=\"PCs\",\n",
    "    value_name=\"covariance\",\n",
    ")\n",
    "\n",
    "# have a look\n",
    "df_plot.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.332868Z",
     "start_time": "2020-02-02T05:45:48.770Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot heatmap of 10 PCs vs top 20 TF KO feats\n",
    "df_plot.hvplot.heatmap(\n",
    "    y = 'PCs', \n",
    "    x = 'Rv_ID', \n",
    "    C = 'covariance',\n",
    "    rot = 70,\n",
    "    clabel = 'covar',\n",
    "    cmap = 'viridis', \n",
    "    hover_cols = ['gene_name', 'Function']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this plot in place, we could drill down and see which TFs have the highest contribution on the variance of the dataset. We'll save them to have them for later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.340833Z",
     "start_time": "2020-02-02T05:45:48.775Z"
    }
   },
   "outputs": [],
   "source": [
    "# get important TFs\n",
    "important_tfs = df_loadings_plot.index.values[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised manifold learning using UMAP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try to do a more powerful method for dimensionality reduction called UMAP (you can read about it more [here](https://umap-learn.readthedocs.io/en/latest/)). The python implementation is well documented, so highly recommend checking it out. \n",
    "\n",
    "Okay, let's see if we can see some sort of structure using UMAP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.343340Z",
     "start_time": "2020-02-02T05:45:48.781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute two-dimensional manifold using UMAP using projections\n",
    "embedding = UMAP(n_components = 2).fit_transform(projections)\n",
    "\n",
    "# Save UMAP coordinates to dataframe\n",
    "df['UMAP1'], df['UMAP2'] = embedding[:, 0], embedding[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.348392Z",
     "start_time": "2020-02-02T05:45:48.787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize using hvplot\n",
    "df.hvplot(\n",
    "    x=\"UMAP1\",\n",
    "    y=\"UMAP2\",\n",
    "    c=\"redox_enzyme\",\n",
    "    alpha=0.4,\n",
    "    kind=\"scatter\",\n",
    "    padding=0.2,\n",
    "    hover_cols=[\"gene_name\", \"Rv_ID\", \"function_redox_\", \"Annotation\"],\n",
    "    cmap=cc.bky, # set colormap using colorcet https://colorcet.holoviz.org\n",
    "    clabel=\"oxired\",\n",
    "    width = 500, \n",
    "    height = 300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some projections comming out of the big blob of data points, but in general, we can see that there is not really a qualitative difference: we still see a big blob without some sort of structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding on denoised data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, let's continue with our exploratory data journey. Now, we'll try to re-apply the UMAP method, but this time instead of doing it on the principal components, we'll do the following: \n",
    ">Using the `pca` object, we're going to reconstruct our data (to the original `(4031, 206)` shape), by using the principal components. This will force the reconstructed dataset to have higher covariance between the features. This can be thought of as \"denoising\" the dataset using a linear transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.360645Z",
     "start_time": "2020-02-02T05:45:48.794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the denoised dataset\n",
    "denoised = pca.fit_transform(projections)\n",
    "\n",
    "# Apply UMAP on denoised dataset\n",
    "embedding_denoised = UMAP(n_components = 2).fit_transform(denoised)\n",
    "\n",
    "# Save the embedding to the original dataframe for visualization\n",
    "df['den_UMAP1'] = embedding_denoised[:, 0]\n",
    "df['den_UMAP2'] = embedding_denoised[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.369000Z",
     "start_time": "2020-02-02T05:45:48.799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "df.hvplot(\n",
    "    x=\"den_UMAP1\",\n",
    "    y=\"den_UMAP2\",\n",
    "    c=\"redox_enzyme\",\n",
    "    alpha=0.4,\n",
    "    kind=\"scatter\",\n",
    "    padding=0.2,\n",
    "    hover_cols=[\"gene_name\", \"Rv_ID\", \"function_redox_\", \"annotation\"],\n",
    "    cmap=cc.bky, \n",
    "    clabel=\"oxired\",\n",
    "    width=500,\n",
    "    height=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh... We still get a giant blob with not too much (aparante) structure. Luckily with UMAP, we still have another *ace up the sleeve*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging UMAP for supervised manifold learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice thing about UMAP is that we can do both supervised and semisupervised manifold learning. What this means is that we can give the labels corresponding to a given (categorical) variable, and the algorithm will optimize the local structure in the high-dimensional space taking into account this variable for \"clustering\". I'm being very loose on the terms here but you can read more about it [here](https://umap-learn.readthedocs.io/en/latest/supervised.html).\n",
    "\n",
    "We'll now do the following: using the variable `redox_enzyme` that is, whether a given gene is classigied as a oxidoreductase or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.375981Z",
     "start_time": "2020-02-02T05:45:48.806Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute UMAP using redox_enzyme labels\n",
    "sup_embedding = UMAP(n_components=2).fit_transform(\n",
    "    denoised, y=df.redox_enzyme.values\n",
    ")\n",
    "\n",
    "# Adding superised UMAP dimensions to dataset\n",
    "df[\"sUMAP1\"], df[\"sUMAP2\"] = sup_embedding[:, 0], sup_embedding[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.382667Z",
     "start_time": "2020-02-02T05:45:48.812Z"
    }
   },
   "outputs": [],
   "source": [
    "df.hvplot(\n",
    "    x=\"sUMAP1\",\n",
    "    y=\"sUMAP2\",\n",
    "    c=\"redox_enzyme\",\n",
    "    alpha=0.4,\n",
    "    kind=\"scatter\",\n",
    "    padding=0.2,\n",
    "    hover_cols=[\"gene_name\", \"Rv_ID\", \"function_redox_\", \"annotation\"],\n",
    "    cmap=cc.bky,  \n",
    "    clabel=\"oxired\",\n",
    "    width=500,\n",
    "    height=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! we can see that by using the supervised version of UMAP we can make visualize the oxidoreductases in the latent space. We should now proceed to see what are the differences between the oxidored in different regions of the latent space. \n",
    "\n",
    "Let's use the Annotation column to get a sense if the oxidoreductases with different annotation quality cluster together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.387080Z",
     "start_time": "2020-02-02T05:45:48.819Z"
    }
   },
   "outputs": [],
   "source": [
    "df.hvplot(\n",
    "    x=\"sUMAP1\",\n",
    "    y=\"sUMAP2\",\n",
    "    c=\"Annotation_int\",\n",
    "    alpha=0.4,\n",
    "    kind=\"scatter\",\n",
    "    padding=0.2,\n",
    "    hover_cols=[\"gene_name\", \"Rv_ID\", \"function_redox_\", \"redox_enzyme\"],\n",
    "    cmap=\"viridis\",\n",
    "    clabel=\"Annot\",\n",
    "    width=500,\n",
    "    height=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, that's it for now. We should investigate further the relationships in different regions of the latent space by using clustering later. \n",
    "\n",
    "Finally we'll color by whether each gene is a TF or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.396551Z",
     "start_time": "2020-02-02T05:45:48.826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot and color by TF \n",
    "df.hvplot(\n",
    "    x=\"sUMAP1\",\n",
    "    y=\"sUMAP2\",\n",
    "    c=\"TF\",\n",
    "    alpha=0.7,\n",
    "    kind=\"scatter\",\n",
    "    padding=0.2,\n",
    "    hover_cols=[\"gene_name\", \"Rv_ID\", \"function_redox_\", \"annotation\"],\n",
    "    cmap=cc.CET_L11[::-1],\n",
    "    clabel=\"TF\",\n",
    "    width=500,\n",
    "    height=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show library versions for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T05:45:53.403072Z",
     "start_time": "2020-02-02T05:45:48.831Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "%watermark -v -p numpy,pandas,bokeh,colorcet,hvplot,seaborn,sklearn,umap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
